# Linear Regression Models for Financial Analysis
In this module, we will explore the most often used prediction method - linear regression. From learning the association of random variables to simple and multiple linear regression model, we finally come to the most interesting part of this course: we will build a model using multiple indices from the global markets and predict the price change of an ETF of S&P500. In addition to building a stock trading model, it is also great fun to test the performance of your own models, which I will also show you how to evaluate them!

# 学習目標
Classify the differences of using covariance and correlation in measuring the association of two random variables
Interpret the association of combination of 2 variables by visualization using python
Give examples of responses and predictors in a simple linear regression model
Illustrate the examples of responses and predictors from financial perspective
Explain the nature of simple linear regression model is to find the best fit line to the mean equation
Use python code in looking for the best fit line in building a simple linear regression model
Analyze the signififace, impact and performance of a simple linear regression model by reviewing the summary using python
Discuss how to diagnotic the validity of the simple linear regression model by checking against the 4 assumptions of linear regression model
Create a signal based trading strategy base on linear regression model
Interpret the major purpose of splitting the stock data into train set and test set in financial analysis
Evaluate the performance of a stock trading model using 2 major financial indicators

# 4.0 Introduction
The value of big data has been widely recognized in every field of business practice, especially in finance industry, because we have the largest data in this field. We have fast-growing of customer data in consumer banks, not to mention that tons of data are generated every moment in offline and online exchanges. Questions also arise about how to utilize this data. Data in financial market is different from those in other fields. They are usually non-stationary and have a high level of noises. Many successful statistical models do not work for this data. Model evaluation is much more important than model building. A statistically successful model is not necessarily successful in terms of profits. In this topic, we will explore the most often used prediction method, multiple linear regression. We will demonstrate how to apply this simple model to achieve success in application - trading stocks. We have five videos to explore this practical topic. We will first introduce association of a random variable and extended to simple linear regression in a second video to make predict Then we present assumptions for linear regression models and demonstrate how to validate these assumptions. In the fourth video, we extend simple linear regression model to multiple linear regression model by allowing more than one predictors. Finally, we apply the model we achieved in trading, most popular ETF - SPY. We will focus on the consistency of performance of models, and how to evaluate correctly. We expect that after this topic, you are able to build your own prediction models. It is great fun to test the performance of your own model using historic data.

# 4.1 Association of random variables
## Association+between+two+random+variables.ipynb
## Association+between+two+random+variables.py
In the previous topics, we only talked about a single variable, but in real life, we may be interested in the association of two or multiple random variables. For example, is there any association or pattern between the stock price change of tomorrow and the number of full days in the last five days? This question is interesting and valuable because stock traders can use this pattern to buy and sell stocks. In this video, we will discuss how to measure the strengths of association between two random variables. Here's an example of housing price. This data concerns, housing values in suburbs of Boston. LSTAT is the percentage of the population classified as low status. INDUS is the proportion of non-retail business acres per town. NOX is the nitric oxide concentrations. RM is the average number of rooms per dwelling. MEDV is the median value of the owner-occupied homes in $1000. In statistics, we use covariance to measure the association between two variables. Here are the formulas to calculate covariance. Similar to sample variance, sample covariance is also divided by the degrees of freedom. You may be interested in the association between each pair variable. We can check that by using method cov of data frame. We can find that from the values, we cannot tell which pair has a stronger association. Indeed, the covariance is also affected by variance of two random variables. We need to factorize it out in order to get a measure only for the strength of association, which is the coefficient of correlation. As you can see in the formula of correlation, the covariance is divided by the standard deviation of both variables. Now, the correlation will only take values in between negative one and one, no matter what are the variation of the two variables. This is a case when there is no correlation. XY pairs on scatter plot looks like a purely random pattern. This is the case why correlation is a positive and very close to one. In this case, X and Y have strong positive correlation. As X increases, Y is more likely to increase. This is the case when correlation almost equal to negative one. Hence, X and Y have strong association, different from positive one, where X increases, Y is more likely to decrease. We also can apply method correlation of data frame. You can find that diagonal elements are one. This is because correlation with itself must be perfectly correlated. In general, there might exist non-linear pattern between variables. Covariance and correlation can only address linear pattern. There are quite a lot of quantitative measure for non-linear association. In this course, we will only use a scatter plot to detect a possible non-linear pattern. This can be done using method scatter matrix, which is imported from tools.plotting of pandas. Scatter matrix is a matrix of scatter plots for each pair of random variables. The histograms in diagonal positions are those of variables. We can find that from this scatter matrix RM seems to have a very strong linear pattern with MEDV, median value of house. Scatter plot or correlation can only find out the association between two variables. It cannot find the association between one variable and other multiple variables. To achieve that, we need multiple linear regression. To illustrate linear regression models more clearly, we will start with simple linear regression, which is a formula or equation built between two variables. We will find this relationship between RM and MEDV, which looks to have a strong association from this plot. We hope we can use RM to predict MEDV so that we can make use of historical data and enhance their value by applying the model in practice.


# 4.2 Simple linear regression model
## Simple+linear+regression+model.ipynb
As I mentioned in previous video, correlation only measures strength of association between two variables. But in practice, we need to evaluate associations between one variable and a set of multiple variables. For example, the house price and other factors, RM and LSTAT. Secondly, in many applications, we not only need to evaluate the strengths of association. For example, in stock trading high-frequency, we may need to estimate price change in next five seconds using information on price change, volume history of last 10 minutes. That is, we need to build a equation between one variable, you try to estimate, called a response and other multiple variables which you use to estimate response called predictors. This equation is called a model. The mostly widely used equation for this is a linear regression model. We use this model to estimate response variable which is also called prediction. In this video, we will discuss linear model using the simplest version - simple linear regression model in which it only has one predictor. From previous exploration, we have the scatter matrix for mutual pattern between variables. We can see that RM - number of roos as predictor, has very strong linear pattern with MEDV, which is the median value of house price. This pair of variables is good for our introduction of simple linear regression model. Just like in confidence interval estimation, we assume the population is normal. For linear regression, we also make assumptions about response and predictors in population. This is our starting point to build the models. We assume that response variable in population are all normal. They have equal variance sigma square. But in the mean of the response variable is determined by predictors. In linear form, mu_i equal to beta_0, plus beta_1, times X_i. Like in confidence interval estimation, we use the samples to estimate a population parameter, mu. Similarly, we also use samples to estimate parameters of population, beta_0, beta_1, sigma. In conclusion, if we apply linear regression model, we assume there exists such a real pattern in population. More specifically, linearity: The mean of y is linearly determined by predictors. Independence: With different X, responses are independent. Normality: The random noise and y follow normal distributions. Equal variance: The variance y are all equal even if the values of predictors are different.
Start transcript at 3 minutes 17 seconds3:17
These assumptions need to be validated if you want to make inference using linear regression models. But in most cases, we do not need to be strict about these assumptions if you just use the model to make a prediction. In real application, we do not know beta_0 which is called intercept, beta_1 which is called coefficient of slope, and sigma neither. So, we cannot identify exact position of a line of a mean equation. Instead, we have samples, consisting of pairs of y and x, as shown in this scatter plot. They are not on a straight line, since the medium price of a house is not only determined by number of rooms, it is also determined by location and other conditions of house, collectively we put all other factors into noise term. It does not mean they are really noise because we will consider their pattern with a house price in a simple linear equation model. We know that mean equation is a straight line. All the sample pair goes around this line because of noise term. Using sample data, our target is to find a straight line y-hat equal to b_0 plus b_1 times x, to approximate mean equation as close as possible, where b_0 is the estimated value of beta_0 and b_1 is estimated value for beta_1. This equation is called a prediction equation. The idea here is a straightforward, but it has a problem. We are not sure which line is the closest to mean equation.
Start transcript at 5 minutes 17 seconds5:17
Since we do not know the location of real mean equation, hence, we have to predict with other standard. Suppose we find a line, with b_0 equal to 1 and b_1 equal to 2. We can view the prediction equation using Python. This prediction equation is our guess of mean equation. GuessResponse is our estimated mean for response, which is also our guess of predict value for response. Then we can compute the difference between real medium price and our guess, which we call observederror. We name it observederror to distinguish it from noise in population model. Observederror can be regarded as an observer of a noise term. Here we print out of observederror for 7's, 20's, 100's pairs from the sample.
Start transcript at 6 minutes 15 seconds6:15
Using method - sum of a data frame, we can compute the sum of squared errors. In short SSE. If SSE is larger, it is easy to figure out, observe pairs, stay far away from your predictor equation. Ideally, we want to get a line whose SSE or sum of squared errors is very small. Alternatively speaking, all sample pairs are close to predict line you found. Now it is turned into a minimization problem. Choose the best estimate values for beta_0 and beta_1 to minimize SSE term. The prediction equation found using this criteria is called best fit line. This process of estimation is called ordinary least square estimation. With a mathematical computation, there exists explicit formulas for b_0 and b_1. From formula of b_1 which is the estimated value for the slope of the model, the denominator is a measure of variation of predictor. The numerator is to measure the association between x and y. The ratio clearly is to measure sensitivity of a change of response with respect to the change of predictor. To estimate models, we have statistical package, Statsmodels. We use method fit of the model OLS of statsmodels. Here OLS stands for ordinary least square estimation. The parameter, data gives the names of data frame of a sample. The parameter formula tells which columns of data frame are response or predictors. We can get estimated coefficients b_0, b_1 using feature params of model. Finally, we can compute predicted value using prediction equation.
Start transcript at 8 minutes 23 seconds8:23
We can plot best fit line along with sample data and our initial guess.
Start transcript at 8 minutes 34 seconds8:34
The yellow color line is a best fit line. Obviously, some square error is much smaller with best fit line. Hence, the criterion using least squares is reasonable. Statsmodels also provide statistical evaluation of the model by using method summary. First, we need to pay attention to p-value of slope. This p-value is a p-value for the two-tail test of a slope. As we discuss in topic three, for population mean, in two-tailed tests, p-value is to compute the probability for statistic to take a more extreme values on the two tails. Here, statistic is b_1 minus beta_1, divided by S_b_1 because population standard deviation for the estimate b_1 is unknown. We use a sample S_b1 for b_1 as a replacement. Hence this statistic follows a t-distribution with degree of freedom, m minus 2. The reason for degree of freedom equal to m minus 2 is because we need the X-bar and y-bar to compute S_b_1 which cause the loss of two freedom. If we can reject the null, freedom y is not equal to zero, it is equivalent to say that then predict RM is useful in predicting the median value of house. We only have five percent chance to include RM in the model wrongly. Second, we can get a confidence interval for the slope. By default, confidence level is 95 percent. Ninety-five percent of confidence interval for the slope is 8.279 and 9.925. This interval is located on the positive side of real line. Hence, the slope is positive with high chance which is consistent with a correlation of a scatter plot of our data. Finally, we need to pay attention to r-square from output of summary. R-sqaure is important measure of performance of a model. First, we will compute the variation of y, without model. which is a sum of square deviation of observed y from the mean of y, which is denoted as SST. Then we can compute the sum of square deviation of predict y, from the mean of y. This is a part of variation response that can be explained by the model, which we denote as SSR. If SSR is larger, the difference between our prediction and the mean of y is larger. It means that our prediction is significantly different from the mean of the response, which is the estimated value of response without model. There is another variation SSE, we mentioned the ordinary least square which is a sum of a square error. It is a variation of a response that cannot be explained by the model or predictor. With a mathematical calculation, this equation holds true. We want a good model in the sense that, most of the changes are our target or response variable, can be explained by our model. In other words we hope SSE the variation unexplained can be relatively smaller. In simple linear regression, we use R square to measure the percentage of explained variation. Here are the model for medium price, R square is equal to 0.484. It means that, about 48.4 percent of variation of MEDV can be explained by our model. Some may ask, is R square equal to 48 percent too low and the model derive is not a good model? We will discuss a more detailed later. Here, please notice two facts. In this example, R square is less than 50 percent. Implies the medium price is not uniquely determined by number of rows. There is a big portion of it, explain by other variables which we need a multiple linear regression. Secondly, if the response is very noisy, like stock return, R square equal to 48 percent is already high enough to generate profit in trading. We will cover these two points in math for linear equation using stock market data.

# 4.3 Diagnostic of linear regression model
## Diagnostic+of+models.ipynb
We have calculated P-value of slope, confidence interval of slope and R square in the previous video using summary method of statmodels. Are they valid claims? In other words, do those assumptions which are necessary for inference hold true? In this video, we will discuss how to diagnose models and validate assumptions of models. To validate four assumptions upon a population, we only need to demonstrate that our sample data is not against these assumptions. Validation of linearity is straightforward, so a scatter plot of Y and X. In our case, scatter plot between medium price and number of rooms had linear pattern. For independence, we just need to demonstrate the observed error is independent mutually. Alternatively speaking, there is no serial correlation in errors. First, we can plot residual plot, which is the plot observed error. There's no obvious pattern in this plot. For quantitative validation of independence, we need a Durbin-Watson test. The Durbin Watson is that, there's no serial correlation in errors. In the output of summary, we can get a Durbin Watson, statistics d=0.684. To find the critical value, it is really cumbersome which depends on sample size and the number of predictors. We have a rule of thumb that test statistic values in the range of 1.5 and 2.5 are rated normal. If below 1.5, it maybe positively correlated. If above 2.5, it maybe negatively correlated, hence an assumption of independence is violated. For normality validation, we can use quantile - quantile plot or QQ plot. We use stats.probplot from python package scipy.stats to draw QQ plot. The first input of probplot is a standardized error. The second term dist = 'norm' is to compare your distribution of standardized error with normal distribution. If errors follow normal distribution, they will fall on the 45 degree line roughly. In our model, it deviates a bit in the right tail, but overall it satisfies the normality assumption. For equal variance, we can plot observed error versus predictor. If variance of noise is equal for different variance predictor, it should not have pattern. In our model, noise variance is smaller for houses with big numbers of rooms. Hence, assumption of equal variance is also violated.
Start transcript at 3 minutes 20 seconds3:20
As we mentioned before, if assumptions are violated you cannot do a statistical inference like testing in a confidence interval. However, the model can still be applied to make a prediction. The accuracy and the consistency of your model, do not rely on these four assumptions. Let us make a conclusion. So far we've discussed simple linear regression model, and it's idea of ordinary least square estimation and the diagnostics of model assumptions. In the next two videos, we will use multiple linear regression models, to build a prediction model for price change of SPY, which is the ETF of S&P500, using multiple global index as the predictors. Finally, we will apply our model in paper trading and evaluate the performance of our models.

# 4.4 Multiple linear regression model
## Multiple+linear+regression+model.ipynb
As we can see in previous model, it is a more natural to include more than one predictor in regression models. The growth of response is determined by multifactors. In this video, we will apply Multiple Linear Regression model to generate a signal for the growth of SPY. The exchange-traded fund, which tracks S&P 500. We finally come to the most interesting part of this course. We will view the model using multiple indices from the global markets and predict the price change of SPY. The reason to choose SPY as a target to view the regression model is because it is very suitable for trading frequently. It is cheap. Each unit of SPY is always approximately one over 10 of S&P 500 index level. To earn SPY, it requires very low fee ratios. Volatility of SPY is very high. Two digits loss and gains race appears often.
Start transcript at 1 minute 14 seconds1:14
We will predict data price change of SPY when US market opens in the morning. We know that different indices in different markets are highly correlated. Using indices for other markets is good for prediction model of SPY. Secondly, different markets are in different time zones. For example, US market opens at 9:00 a.m and close at 4:00 p.m Eastern time today. Europe markets open at 3:00 a.m and close at 11:30 a.m Eastern time on the same day. That is, when US market opens, update data of Europe market is available. Asian markets including Australia open at around 8:00 p.m Eastern time yesterday and close at 3:00 a.m Eastern time today. That means the Asian market information is available for US market at its opening. In this video, we will use aud ordinaries from Australia, Nikkei from Japan, HSI from HK, DAX from Germany, CAC40 from France, S&P 500, Dji, and Nasdaq from US market, to predict the daily price change of SPY. In the next video, we will demonstrate how to use Prediction Model in paper trading. That is to trade using historical data. This data can be downloaded from Yahoo Finance easily. We did not use FTSE 100 from UK, which definitely is an important index, because it is now not available for downloading from Yahoo Finance. You use data frame from CSV to read assessing files of all indices and SPY. All data sets have six columns. Open is a price at the beginning. High, Low are the highest and lowest prices on that day. Close is a price at the chloro. Adjust close price is adjusted closing price that has been amended to include any disputions and corporate actions that occurred at any time before the next day's open. Volume is the number of shares traded on that day. In our demonstration, we will only use open price to simplify our discussion.
Start transcript at 3 minutes 47 seconds3:47
Different from Simple Linear Regression, Multiple Linear Regression will have multiple predictors. Our response variable is open price of SPY tomorrow minus today's open. With this response, we expect to make a prediction in the morning in US market. Based on predict price change, we decide whether to long or short. Here, totally, we have eight predictors. Just a reminder, we cannot use any information available after the opening of a US market on current day to calculate predict values. In other words, these variables cannot be predictors. We will have three groups of predictors. The first group is a one-day lag variables from US market. Open minus open price of last day for SPY, Sp500, Nasdaq, and the Dji. The second group is a one-day lag variables from European markets. Open minus open price of last day for Cac40 and the Daxi. Ideally, for European markets, we want to use price at noon minus open price. If you have intraday data, you can improve this model. However, Yahoo Finance does not provide intraday data. Last group, close price minus open price of Aord, HSI, and the Nikkei in Asia, Australian markets. Next, we will mung the data to get all these predictors and the response. First, we generate an empty data frame and let the index to be the same as the index of SPY. Then, we add the contents of response and predictors we defined in last slide. Notice, in the last row of close, we keep a record of open price of SPY, which will be used in the next video. If we print the head of this table, we find missing values. This is due to two reasons. When we calculate price change, we may generate NaN value in the first row, one-day lag, and the last row, one-day in the future. In different markets, they may have different holidays in which the markets are closed. It can be shown by computing numbers of NaN values in each column. We find Australia markets seems to have more holidays. We need to handle NaN values first before we view the model. First, we use fill-forward method to fill the holes of data frame by propagating last valid observation forward to next valid. Second, we drop the first row by using dropna. We can check if there is any NaN remaining by computing numbers NaN values. From the printout, it is clear that we fill all holes. Finally, we save our clean data into data file indicepanel.csv using method to_csv of DataFrame. We will use this data in next video. We can print the measure of the data. Totally, we have 2,677 days of data, one response variable, nine predictors, and the last column keeps a record of open price of SPY, which will be used in paper trading. To make sure that our model is consistent in future data, we need to split data into two parts; one is for building the model, the other part is for testing the model to see if the model can still make reasonable prediction in this dataset. Stock data is very noisy comparing to other static data like images. We will use the equal size of samples for both train and test. We assign the most recent, 1,000 days at the test data and 1,000 days before the test at the training data. Firstly, we use a scatter matrix to get a pairwise scatterplot. If you check the scatterplots, which will response SPY with other nine predictors. You may find that, there is no explicit pattern, which is evidence of high noisy properties of stock markets. We need to compute correlation in order to see the association clearly. From the output of correlation, we find that the predictors for Europe and Asian markets do have association with SPY, which have higher impacts than predictors of U.S. markets. We can use OLS method of Statsmodels to build multiple linear equation model. The summary is printed out. There are a couple things we need to pay attention; the first thing is a p value for F-statistics. F test is for overall significance of the multiple linear equation model. If we reject, we accept alternative and it means that at least one of the predictors is useful. Our model is better fitted than intercept only model. P-value equal to 0.0106 in our model, which is less than 0.05 and it indicates that, our model includes useful predictors. Summary table also lists the p value for the test of significance of the individual predictors. From last video, we know that p value of test statistic used for this test. For our results, we find that, most of the predictors are not significant, except the Aord. That means all other predictors are useless information of SPY. It may be because of multicollinearity.
Start transcript at 10 minutes 25 seconds10:25
Multicollinearity refers to a situation in which two or more predictors in the multiple regression model are highly, linearly related. One predictor can be predicted from the others with a substantial degree of accuracy and it is typical for our model since all indices of different markets are correlated. Multicollinearity does not reduce predictive power. In this situation, the coefficient estimates of the multiple equation may change erratically in response to small changes of data. Now, we can predict daily change of SPY using message predict of our model LM. We predict SPY in both train and test. We can further ensure scatterplot between real daily change and predict daily change of SPY. It does have positive correlation although not very strong. Considering it is daily change, this result is not very bad. Next, we will evaluate our models by comparing two statistics in train and test.
Start transcript at 11 minutes 44 seconds11:44
First statistic is RMSE, which is the square root of sum of squared errors averaged by degrees of freedom, where k is number of predictors. This statistic is to measure the prediction error. The reason to use the degrees of freedom is that, square of RMSE is unbiased estimator of variance of the noise. The second is adjusted R-square. In Simple Linear Regression, we use R-square to get the percentage of variation that can be explained by a model. We found that by adding more predictors, the askew is always increasing, but the accuracy is even worse. To compensate the effects of numbers predictors, we have adjusted R-square, which measures percentage of variation of a response that is explained by the model. We will compute R-squared and RMSE in many practice and we do not want to run a similar course repeatedly. In python, the compacted this course under a function name, so that we use this course repeatedly just by referring to the function next. The keyword def is a keyword to define a function and return its keyword to offload your result from the inside of function. Functions to compute adjusted R-squared RMSE is given, where the parameter model is a model name and model k is the number of predictors. Y name is a column name of our response variable. Then, we compute adjust R-squared and RMSE for post train and test using function adjust metric.
Start transcript at 13 minutes 39 seconds13:39
We give another function access table to generate output, which compared to R-square and RMSE between train and test. We compute RMSE and adjust R-square in both train and test in order to check whether they are different dramatically. If so, this is called overfitting. Usually, for overfitting model, RMSE and adjusted R-square is much better in train than in test dataset. That it implies that we cannot apply this model to real market in future. From output our model, RMSE increases in test, which is a bit worse than that in the train. I just ask where the test is batter. Overall, our model is not overfitted. Our R-square is only 1.5 percent, which is quite low, but in stock market it's not that bad. In next video, we will use predictive value of our model as a signal to buy and sell SPY in both train and test to check whether the signal could generate profit consistently.

# 4.5 Evaluate the strategy
## Evaluating+strategy+built+from+Regression+model.ipynb
In the last video, we have viewed a multiple linear regression model for the price change of SPY and find the model is not overfitted by comparing adjusted R squared and RMSE in train and test datasets. In this video, we will use predict price change of SPY as a trading signal and then perform a simple strategy. If the signal is positive, we were long. Otherwise, we were shot. First, we'll compute a position of our trading based on our predicted value of a response. Order equal to one if predicted value is positive or our prediction for price change is positive for opening today to opening tomorrow. Otherwise, order equal to negative one, which means we will sell one share if we had one share, and then short sell one share. Daily profit is computed in column is profit. Finally, we can compute a cumulative profit and sum it in the column of wealth. Total profit in this 1000 days is $170. Then, we can compare the performance of this strategy, which we call the signal-based strategy, with a passive strategy, which we call buy and hold strategy, which is to buy more shares of SPY initially and hold it for 1000 days. We can see from the plot, signal-based strategy outperforms buy-and-hold strategy. Similarly, we can view it as trading in test dataset, and the total profit is $130, a bit lower than that in train. Signal-based strategy also outperforms buy-and-hold strategy in the test data. The consistency of performance is very important. Otherwise, it is too risky to apply it in the future. Average daily return is a mirror we can make comparison in finance industry when they use a Sharpe ratio and maximum drawdown.
Start transcript at 2 minutes 21 seconds2:21
Sharpe ratio measures excess return per unit of deviation in an investment asset or trading strategy named after William Sharpe. For example, daily Sharpe ratio is equal to the mean of excess return divided by standard deviation of excess return. Since there are about 252 trading days per year in US stock market, the yearly Sharpe ratio is equal to daily Sharpe ratio multiplied by square root of 252. To compute the Sharpe Ratio, we first need to revise wealth process by including initial investment which is the price of one share of SPY. We will use 'loc' version for return process. Yearly Sharpe ratio is 2.04 for training data and 1.184 for test data which is a much lower than that of the train. Maximum drawdown is a maximum percentage decline in the strategy from the historical peak profit at each point in time. We first compute drawdown, and then the maximum of all drawdowns in the trading period. Maximum drawdown is that risk of mirror for extreme loss of a strategy. To compute a drawdown, we need to compute peak of wealth process. At every time point, we can do it easily using data frame method, cummax. We find maximum drawdown in train and in test are very close. The result shows that if we apply this strategy in test set, the maximum loss from the peak is nine percent. From the mirror of a Sharpe ratio and maximum drawdown, we can tell that the performance of strategy is quite consistent in place of extreme loss. But the return per unit risk is not very consistent.
Start transcript at 4 minutes 43 seconds4:43
In this topic, we have explored linear regression model mainly from perspectives of statistics. We then trained the application of a linear regression model in SPY trading. We conclude that the derived model is not overfitted and the performance of signal-based strategy is consistent in terms of risks of large loss. It is promising to further develop it into a profitable strategy, but they may cost a lot of efforts. Although the phase ratio of SPY trading is very low, the bid-ask spread is a hidden cause for this strategy and may eat up all profits. But it is not the objective of this course. In this course, we then traded that with adaptive learning skills in Python and the core concepts in statistics, we can explore the real finance problems by interacting with the real data.
