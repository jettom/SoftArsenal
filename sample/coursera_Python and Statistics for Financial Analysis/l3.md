# Sampling and Inference
In financial analysis, we always infer the real mean return of stocks, or equity funds, based on the historical data of a couple years. This situation is in line with a core part of statistics - Statistical Inference - which we also base on sample data to infer the population of a target variable.In this module, you are going to understand the basic concept of statistical inference such as population, samples and random sampling. In the second part of the module, we shall estimate the range of mean return of a stock using a concept called confidence interval, after we understand the distribution of sample mean.We will also testify the claim of investment return using another statistical concept - hypothesis testing.

# 学習目標
Compare the properties of population and sample
Illustrate the difference between two kinds of sampling with examples
Explain the use of unbiased estimator (n-1;ddof=1 in python) when calculating sample variance
Describe the distribution of sample mean and variance of normal distributed population
Use the central limit theorem to explain the distribution of sample mean of aribitary population
Explain the implication of Confidence Interval in estimating average stock return
Identify the basic concept of Confidence Interval in estimating population mean
Outline the steps involved in performing hypothesis testing for validating assertion about population
Apply the steps involved in hypothesis testing in testifying the claims of investment return
Recognize p-value as an alternative quatitative tool in performing two tail test - a part of hypothesis testing

# introduction
Hello everyone, welcome back to the course. In previous topic, we have discussed random variables. We know that, when doing financial analysis, identifying important variables is important in helping us making better predictions and decisions. We have also learned the distribution of random variables. We learned how to apply normal distribution to model a stock return, and evaluate the risk of extreme events. In this topic, we will explain statistical inference, which is a core part of statistics. In financial analysis, we're concerned about characteristics of some targets called a population.
0:50
For example, we want to make use of historical data of a couple years, called sample, to estimate the real mean return of some private equity funds. Sometimes, we also want to testify some claims. For example, if the fund managers claim that their investment strategy can generate 30% yearly return, we have to validate this claim using data of the last 20 years.
1:21
These two applications are typical tasks of statistical inference to infer the promptings of interesting targets. We are going to explore this topic in the following four videos. First, we will talk about population, samples and random sampling. In the second video, we will find out the distribution of sample mean. In the third video, we will learn how to use confidence interval to estimate population mean, for example, the average data return. In the last video, hypothesis testing, will be introduced and applied in validating some claims about the mean return.

# 3.1 Population and Sample
## Population+and+Sample.py
## Population+and+Sample.ipynb
[MUSIC] In this video, we are going to discuss basic concepts about population and sample. Population is a group of individuals who have something in common. We may be interested in some properties about a certain group, which we call target population, that cannot be observed completely. For example, all registered voters in Thailand, or all Hong Kong citizens who played golf at least once past year, or all neurons from your brain.
0:43
Since we cannot get information for every individual of these populations, we have to take sample, which is a part of target population.
0:54
Sample is a small group of population. It is a representative of the population, hence, it has to be randomly selected. This process is called random sampling.
1:08
There are two kinds of sampling, called a sampling without replacement, or with replacement, depending on whether you put the select individuals back into population before you select the next one.
1:25
In a sampling without replacement, an individual is randomly selected from the population. The next individual will be selected without putting the previous individual back.
1:38
Every individual is selected from a different pool. If the population size is very large, this method is a factor because it can generate a random sample with different individuals.
1:53
There is another kind of sampling method, sampling with replacement.
1:59
A randomly selected individual will be put back before the next individual being selected. Hence, it is possible that the same individual can be selected more than once.
2:13
When the population is small, this method can make sure that everyone in the population has the same chance being selected.
2:24
In this example, we first create a blank DataFrame and we create a new column in this DataFrame.
2:33
Name the column as Population. Then I run this python code to perform sampling from the population by drawing five individuals randomly without replacement. And stored them, as a_sample_without_replacement. In we can use method sample to get samples. Here, the key word replace=False means the sampling is a sampling without replacement.
3:07
We can define replace True to perform sampling with replacement.
3:14
Here are the results of two samples. You can see that for sampling without replacement, which is on the left-hand side, there are no duplicates individuals found.
3:28
As we can see, there are no duplicate index numbers either. However, on the right-hand side, which is the sampling with replacement, it is possible to see the same individual is drawn for multiple times.
3:47
Besides population sample, there is another pair of concepts, parameter and statistics, of which are very important in categorizing population and sample.
4:00
Parameter is a characteristic or summary number of populations. Statistic is a characteristic or summary number of sample. For example, mean, variance, standard deviation of population are characteristics of population.
4:16
Once target populations are fixed, these summary numbers will not change.
4:21
Sample also has mean, variance, and standard deviation, but their values changes in different samples, even if they are drawn from the same population.
4:35
Let's print out the parameters of population which we have just created.
4:41
We can calculate the mean, variance, and the standard deviation using dataframe methods, .mean, .var, .std. The mathematic formulas for mean and variance are given.
4:56
In calculating var and std, we set the keyword ddof as 0. Which means the denominator of the population variance is N, which is the number of the population. The .shape, as described in the previous module, offers a size of the data frame in rows and columns. We use .shape(0) to display only the number of rows.
5:24
Also, these methods can be applied to compute statistics of samples. First, let us get a sample with size 10 from population, using replacement. The sample and sample variance standard deviation are printed out. Notice that when we compute a sample variance and std, we need the ddof equal to 1. It means the denominator of a sample variance has to be n-1 instead of n, which is the sample size.
6:03
To understand why the denominator of the sample variance is n-1, let us generate 500 samples, each sample with size equal to 50. We calculate sample variance for each sample using n and n-1 as denominators. You'll find that the real population variance is 571.8. As you can see, the average of sample variances using n-1 is closer to population variance than using n.
6:40
The average of sample variance using n as denominator is always smaller than population variance, which is some mathematical reasons. n-1, in fact, is called degrees of freedom. For sample variance, there is another explanation why sample variance is divided by n-1, which is the degrees of freedom. Degrees of freedom is a number of values used in calculation that are free to variate. When we use the sample mean to compute the sample variance, we will lose one degree of freedom.
7:20
For example, sample size equal to 3, sample mean equal to 100 is used in computing sample variance. We can choose two individuals independently, for example, 90 and 80 in the table, but the third is not free to variate because of constraint of sample mean.
7:42
The only choice for a third number is 130. When we compute the variance, using degrees of freedom is better than using sample size. In the sense that the average of estimator will be equal to population variance, which is called an unbiased estimator. In the next video, we will talk about distribution of sample statistics.

# 3.2 Variation of Sample
0:00
In this video, we will talk about the variation of sample mean and the distribution of the sample mean. Knowing the variation and its rule is important to have us correctly evaluate the estimation, and validate assertions about population based on the samples. For example, you have historical data of 100 days. You can compute sample mean, and variance of stock return. Based on the statistics, can we show inference to the parameters? How close are the statistics to population parameters? By observing the stock data of 100 days, can we make a claim that this stock is in a upward trend? That is, mean for return is positive. All these rely on our understanding of sample mean distribution.
1:06
Here, we take a sample, when sample size equal to 30, from a population with a normal distribution, mean equal to 10, and a standard deviation equal to 5. If you run this cell several times, you make a different result from there. For example, like this, like this. It is because, the samples are randomly drawn from a normal distribution. Different samples will yield different means and standard deviation. This is called the variation of sample. Furthermore, the sample mean and the standard deviation do not change arbitrarily. It also follows some rules, because they are all taken from the same population. To see that, in this code, we generate 1,000 samples from the same population. We got mean and variance for each sample and saved in a DataFrame collection. Meanlist is a name of a list to same sample means of 1,000 samples. Varlist is a name of list to save sample variances of 1,000 samples. Then, we will generate 1,000 samples in a loop. For each sample, we compute the mean and variance and save them into meanlist and varlist. Finally, we build an empty DataFrame called collection, the same meanlist and varlist in different columns of this DataFrame. We can draw a histogram for the collection of sample means. It looks symmetric and like a normal distribution. The histogram of sample variance is not normal as you can see it is right-skewed. We can guess, in fact, we can mathematically prove that the sample mean has a normal distribution. If population is normal with mean equal to Mu and variance equal to sigma square, then the sample mean is also normal, with mean equal to Mu and variance equal to sigma square, divided by sample size N. Why variance of the sample mean is smaller than variance of a population? Intutionally, the sample mean is the average of N individuals of population, and hence the variation of sample mean is smaller than the variation of individuals in population. Here is a demonstration using Python. Then blue histogram is for population, the red one is for the sample mean. What if the population is not normal? Central limit theory of statistics say, if the sample size is large, the distribution of sample means looks like normal one. Hence, we can conclude this way, even if the population is not normal, the sample is approximately normal if the sample size is large enough. Here's an example about distribution of sample mean when the population is not normal. As you can see here, apop is a DataFrame name, which save the population. In this population, we only have five values, one, zero, one, zero, one. We can generate 100,000 samples with small sample size 10. You can see that in this histogram, for sample means, it does not look like a normal distribution. But, if you generate 100,000 samples with large sample size 2,000, the distribution of sample mean now looks like a normal distribution. In this video, we mainly talked about distribution of sample mean, we described probability rule of variation of sample mean. We will apply these in next two videos, to explore two important quantitative statistical tools, confidence interval, and hypothesis testing.

# 3.3 Confidence Interval
In this video, we will explore how to estimate the average return using confidence interval. Here is a sample of the log return of a stock price of Apple. We can get average return in this sample. We can sample mean to estimate the real average return, which is population mean in our example. We want to do more about estimation. Intuitionally, if a sample is a good representative of the population, the population mean should be close to sample mean. It is plausible to say that the population mean is in a range with sample mean centered. Hence, our task in this video is to estimate population mean using interval with lower and upper bound. To start with, we need to standardize sample mean because different sample has different mean and a standard deviation. We have learned that distribution of sample mean is normal in our last video. We can standardize sample mean by minus it's mean, which is identical to population mean and then divided by its standard deviation, which is the standard deviation of population divided by square root of sample size. After standardization, it'll become standard normal, and follows Z-distribution. For Z-distribution, it is not difficult to find the two quantities; Z Alpha over two, and Z, one minus Alpha over two such that in the middle, the probability is one minus Alpha. Since Z is symmetric with respect to zero, the magnitudes of these two quantiles are the same except the sign. We have Z_1 minus Alpha over two equal to negative Z Alpha over two. Since the standardization form of sample mean is also Z, then we have this equation. With some calculation in the parenthesis, we can have this form, which explicitly gives the upper and lower bounds for population mean. Notice that Sigma is the population standard deviation, which is usually unknown. In practice, we can replace it using the sample standard deviation if sample size is large enough. The interval here, for Mu is called confidence interval at the level of one minus Alpha. In our problems, to build the interval for the average return, we need to find quantiles of mean distribution which has been discussed in topic two. We can use the norm.ppf to get the quantiles. Here, alpha over two is equal to 0.1, hence confidence level is 80 percent. Then we can calculate sample mean and the standard deviation of sample in which the population standard deviation is replaced by sample standard deviation. The 80 percent of confidence interval is printed out. Average return of Apple stocks falls in this interval with 80 percent chance. Notice, this interval is on the positive side. It implies that the average return is very likely to be positive. In next video, we will discuss another statistical inference tool, hypothesis testing, which can be applied to detect difference or change of population.

# 3.4 Hypothesis Testing
0:00
[MUSIC] In many situations, we need to demonstrate validity of assertions. For example, you are a venture capitalist and is proposed a project running 36 months.
0:19
With 36 months data at hand, should you invest in this project? Suppose you will invest if average monthly profit is over 20,000. This question is not to ask you to estimate some parameters. Instead, you need to make a judgement whether the condition is satisfied. We need a new statistic tool, hypothesis testing.
0:48
This is a daily close price of Apple from 2007 to 2018. It looks like the price is in an upward trend and we may guess the average of daily return is positive. However, if we plot the daily return directly, the daily return goes positive, negative. And our assertion that the average of daily return is positive is not obvious.
1:14
It is also not obvious whether the average of daily return is 0 or not.
1:20
In the histogram daily return, it is approximately symmetric above 0. It is still not obvious whether the average daily return is different from 0. We want to use a quantitative statistical tool to make judgement about the assertion that the average daily return is not 0.
1:40
In statistics, hypothesis testing can use sample information to test the validity of conjectures about these parameters.
1:53
Let's start with hypothesis testing. The first step is to set hypothesis. We have null hypothesis and alternative hypothesis. Usually, the null hypothesis is assertion we are against. Alternative hypothesis is a conclusion we accept whenever we reject the null.
2:17
Hence in our example, the null is population mean, the average daily return is 0. In alternative hypothesis, average daily return is not equal to 0.
2:31
Whether to reject or not reject null hypothesis is a sample based decision. Intuitionally, given that the null is correct, the difference between sample statistic, x-bar, and the population parameter mu cannot be very large. If it's significantly large, the null should be incorrect, and we should accept alternative.
2:57
To measure the magnitude of difference, we also need to consider the standard deviation of the sample, because the sample with a large standard deviation usually its magnitude is larger, hence we need to do standardization.
3:14
If we know population standard deviation, the standardized sample mean we denote as z-hat, and the z-distribution, if population is normal, or it's sample size large. If a z-hat is far away from 0, then null is not likely to be true. In hypothesis testing, we start with assumption that the null is correct. Hence, we know population mean is equal to 0. But in most situations, population standard deviation is not known. Then we can replace population standard deviation with the sample standard deviation. Then this new term denoted as t-hat, has a new distribution, t-distribution.
4:00
Similar to z-distribution, t is a symmetric with respect to 0 and a bell-shaped. The difference lies in the tail. T-distribution had flat tails which implies t has a higher chance to take the values in the two tails. The t-distribution is a dependent on the degree of freedom. In our example, the degree of freedom is equal to the degree of freedom of the sample standard deviation, which is n-1. As the sample size increases, the degree of freedom increases, and the t is more and more like z-distribution. So with a large sample, we can treat t as if it is a z-distribution.
4:47
We will use a t-distribution in next topic.
4:53
With large sample, t-hat follows z-distribution hence we denote this statistic using z-hat too. To emphasize that, it follows z-distribution.
5:05
We can compute z-hat using Python. X-bar is sample mean, average daily return. S is sample standard deviation, n is a sample size.
5:16
Z-hat equal to 2.5897 It's a standardized statistics. We use mu equal to 0 because we assume the null is correct at the beginning.
5:28
If z-hat is different from 0 significantly, we can infer that this sample is not sampled for the population with mean equal to 0. Then, we can reject the null. How do we find the significance level? We use the probability on the two tails of a z-distribution. we fixed the significance level, for example, alpha equal to 5%, which is the probability for z to take the values of some demands of the two tails? We can see that these two demands is z less than -1.96 and z bigger 1.96. -1.96 and positive 1.96 are 2.5% and 97.5% quantiles of z. These two demands are called rejection regions and this kind of test is called two-tailed test. If statistic z-hat falls into rejection region, we can tell that statistics is far away from 0, significantly and then we can reject the null. Here, we should notice that z-hat is also possible to take values in rejection region even if the null is correct, and the mu equal to 0. This chance is equal to alpha 5%. In other words, we have a 5% chance to reject null wrongly. This is called a type 1 error, and the probability of a type 1 error is identical to the level of significance level.
7:12
If a significance level is small, the probability of a type 1 error is smaller.
7:19
Here, we demonstrate how to get the quantiles which is also called critical values. Alpha equal to 5% is a given hence, norm.ppf can be applied to get the quantiles. In the print, we use a bold number to generate whether to reject or not directly.
7:42
Hence, our final conclusion, we will reject null hypothesis and the average daily return is not equal to 0. Our conclusion may be wrong but it happens only with 5% probability. We may want to further demonstrate that the average return is in fact positive. We need another kind of test, one-tail test.
8:11
In one tail test, the null is the average daily return is less than or equal to 0, which we are against. The alternative hypothesis, the average daily return is positive. First, we take a mu equal to 0. We still need to standardize sample mean, which is average daily return in the sample.
8:38
If z-hat is significantly large, which implies that sample mean is a positive comparing to mu equal to 0. Hence, it is not likely to be sampled from population, which may equal to 0. It is also not likely to be sampled from population with negative mu.
8:59
Similarly, we fix significance level alpha, and identify rejection region using z-distribution, which is z-hat is larger than z-alpha.
9:13
Using Python, we can show that the null is rejected under 5%. It means that the average daily return of a population is indeed positive.
9:25
From this result, we do need a quantitative statistic tool to validate our assertion in addition to visualize the data. Conclusion,
9:37
For population mean, we have these three kinds of hypothesis in the regression criteria.
9:44
For different kinds of hypothesis, the criterion is different.
9:49
A more popular way of testing is to compute p-value. We know that given that null is correct, standardized sample mean follows z-distribution. What is the probability for this distribution to take a more extreme value than our observation in given sample? This is a p-value, if p is less than alpha which is a threshold, it means that the null is unlikely to be true.
10:23
With p-value, we only need to compare it with alpha although the way to compute p is different. Here's a demonstration of p-value approach, in two-tailed test, abs is to compute the absolute value. We use norm.cdf to compute cumulative probability.
10:47
In this video, we discussed another quantitative tool which is applied to validate assertion about population. In next topic, we will use all knowledge we learned in topic two and three to explore relationship among different variables to build a prediction model in stock market. And finally, evaluate the performance of our models.